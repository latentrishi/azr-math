import logging
from typing import Optional, Dict, Any, Tuple

from transformers import AutoModelForCausalLM, AutoTokenizer # For type hinting and example

from math_azr_reasoner.data_construction.prompts import (
    PROPOSER_SYSTEM_PROMPT,
    PROPOSER_TASK_PROMPT_TEMPLATE,
    PROPOSER_FEW_SHOT_CONTEXT_HEADER 
)

logger = logging.getLogger(__name__)

class ProposerClient:
    def __init__(self, model: Any, tokenizer: Any, generation_params: Optional[Dict[str, Any]] = None):
        """
        Initializes the ProposerClient with a local Hugging Face model and tokenizer.

        Args:
            model: The loaded Hugging Face model (e.g., AutoModelForCausalLM).
            tokenizer: The loaded Hugging Face tokenizer (e.g., AutoTokenizer).
            generation_params: Dictionary of parameters for model.generate()
                               (e.g., max_new_tokens, temperature, do_sample).
        """
        if model is None or tokenizer is None:
            raise ValueError("Model and Tokenizer must be provided for ProposerClient.")

        self.model = model
        self.tokenizer = tokenizer
        self.generation_params = generation_params if generation_params else {}
        # Set default generation params if not provided
        self.generation_params.setdefault("max_new_tokens", 150) # Questions are shorter
        self.generation_params.setdefault("temperature", 0.8)
        self.generation_params.setdefault("do_sample", True)
        self.generation_params.setdefault("top_k", 50)

        if self.tokenizer.pad_token_id is None and self.tokenizer.eos_token_id is not None:
            logger.info(f"Tokenizer does not have pad_token_id, setting to eos_token_id: {self.tokenizer.eos_token_id}")
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id
            if hasattr(self.model, 'config') and self.model.config is not None:
                 self.model.config.pad_token_id = self.tokenizer.eos_token_id

    def get_proposer_question(self, context_string: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Generates a new math question based on the provided context string.

        Args:
            context_string: A string containing formatted examples of existing questions
                            and their solve rates, typically generated by MathTaskBuffer.
        """
        # The PROPOSER_TASK_PROMPT_TEMPLATE expects {few_shot_context}
        # The context_string already includes the PROPOSER_FEW_SHOT_CONTEXT_HEADER
        # if it's generated by the buffer's sampling method correctly.
        prompt = (
            f"{PROPOSER_SYSTEM_PROMPT}\n\n"
            f"{PROPOSER_TASK_PROMPT_TEMPLATE.format(few_shot_context=context_string)}"
        )

        try:
            logger.debug(f"Generating proposer question with context:\n{context_string}")
            logger.debug(f"Using generation params: {self.generation_params}")

            inputs = self.tokenizer(prompt, return_tensors="pt", padding=True, truncation=True)
            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}

            gen_kwargs = self.generation_params.copy()
            if 'pad_token_id' not in gen_kwargs and self.tokenizer.pad_token_id is not None:
                gen_kwargs['pad_token_id'] = self.tokenizer.pad_token_id
            if 'eos_token_id' not in gen_kwargs and self.tokenizer.eos_token_id is not None:
                gen_kwargs['eos_token_id'] = self.tokenizer.eos_token_id

            output_sequences = self.model.generate(
                input_ids=inputs["input_ids"],
                attention_mask=inputs["attention_mask"],
                **gen_kwargs
            )

            prompt_input_ids_length = inputs["input_ids"].shape[1]
            generated_ids = output_sequences[0][prompt_input_ids_length:]
            new_question = self.tokenizer.decode(generated_ids, skip_special_tokens=True)

            logger.debug(f"Raw generated output from Proposer: {output_sequences}")
            logger.debug(f"Decoded new question from Proposer: {new_question}")
            return new_question.strip(), prompt
        except Exception as e:
            logger.error(f"An error occurred while generating proposer question: {e}", exc_info=True)
            return None, None
